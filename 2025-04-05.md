好的，同学们！前面我们聊了词嵌入、语言模型、序列标注等，这些很多都依赖复杂的深度学习模型。今天我们稍微回溯一下，看看在深度学习大放异彩之前，处理序列问题（尤其是带隐藏状态的）的一个非常经典且重要的统计模型——**隐马尔可夫模型 (Hidden Markov Model, HMM)**。

理解 HMM 对于了解 NLP 序列建模的演进非常有帮助，而且它的思想（比如状态转移、发射概率）在后续更复杂的模型中也能看到影子。别担心数学，我们还是主打直觉和比喻！

---

## 概念名称：隐马尔可夫模型 (Hidden Markov Model, HMM)

### 应用场景（为什么需要）

*   **要解决什么问题？**
    想象有很多现实世界的问题，我们能观察到一系列**可见的事件（观测序列）**，但我们相信这些可见事件是由一个**隐藏的、我们无法直接看到的内部状态序列**生成的。我们想根据观测到的序列，**推断**出背后最有可能的隐藏状态序列，或者计算这个观测序列出现的**概率**。
    *   **NLP 例子：词性标注 (POS Tagging)**
        *   **观测序列 (可见):** 句子中的单词序列，比如 "Time flies like an arrow."
        *   **隐藏状态序列 (想知道):** 每个单词对应的词性标签序列，比如 "Noun Verb Prep Det Noun." （时间/名词 飞逝/动词 像/介词 一个/限定词 箭/名词）
        *   我们只看到单词，词性是隐藏的，需要推断出来。
    *   **其他例子：**
        *   **语音识别：** 观测到的是声音信号（声学特征序列），隐藏状态是对应的音素或单词序列。
        *   **生物信息学：** 观测到的是 DNA 碱基序列，隐藏状态可能是基因编码区或非编码区。
        *   **金融：** 观测到的是股票价格序列，隐藏状态可能是牛市、熊市或震荡市。

*   **以前怎么做？有什么局限？**
    在 HMM 之前：
    1.  **简单统计/规则：** 比如只看一个词最常见的词性是什么，或者写规则“‘the’后面很可能是名词”。
        *   **局限性：** 忽略了词语在**序列中的依赖关系**（比如动词后面可能跟名词或介词，但不太可能跟另一个动词），规则难以覆盖所有情况。
    2.  **（如果不用 HMM 的思想）** 很难有一个统一的、概率化的框架来同时建模：
        *   隐藏状态之间的**转移规律**（比如，限定词后面接名词的概率）。
        *   隐藏状态**生成观测值**的规律（比如，名词状态下生成单词 "cat" 的概率）。
        *   如何根据观测序列**找到最优**的隐藏状态序列。

*   **为什么需要 HMM？**
    HMM 提供了一个**数学框架**来解决这类涉及**隐藏状态**和**观测序列**的问题。它能：
    1.  **形式化地定义**隐藏状态、观测值、状态转移概率、观测生成概率（发射概率）和初始状态概率。
    2.  提供**高效的算法**来解决三个核心问题（后面会讲）：
        *   **评估 (Evaluation):** 计算某个观测序列出现的概率。
        *   **解码 (Decoding):** 找到最可能产生该观测序列的隐藏状态序列。
        *   **学习 (Learning):** 根据观测数据估计模型的参数（各种概率）。

*   **不用 HMM 会怎样？**
    对于需要推断隐藏状态序列的任务（如词性标注），我们将缺乏一个强大且经过理论验证的概率模型。可能需要依赖更启发式、效果可能不稳定的方法，或者需要大量人工设计的特征（像 CRF 那样，但 HMM 比 CRF 更早，结构更简单）。在深度学习方法普及之前，HMM 是这些任务的主流且效果不错的方法之一。

### 是什么（概念定义及原理 + 数学直觉）

*   **定义：**
    隐马尔可夫模型 (HMM) 是一个关于**时序**的**概率模型**。它描述了一个系统，该系统包含**无法直接观测到的隐藏状态 (Hidden States)**，这些状态随时间按一定的概率（满足马尔可夫性质）进行**转移**，并且在每个时刻，处于某个隐藏状态的系统会按一定的概率**生成（或称发射 Emit）**一个**可观测到的输出 (Observation)**。

*   **核心思想与比喻 (敲黑板！理解这个比喻就懂了大半！)：**
    **想象一个房间里有个“天气精灵”（我们看不到它，它是隐藏的）**。
    *   **隐藏状态 (States):** 天气精灵每天的心情只有两种：**开心 (Happy)** 或 **伤心 (Sad)**。这就是隐藏状态集合 $Q = \{\text{Happy, Sad}\}$。
    *   **观测值 (Observations):** 我们在房间外，只能看到精灵每天选择的**活动**：**唱歌 (Sing)**、**跳舞 (Dance)** 或 **哭泣 (Cry)**。这就是观测值集合 $V = \{\text{Sing, Dance, Cry}\}$。
    *   **初始状态概率 ($\pi$):** 精灵第一天是开心的概率是多少？是伤心的概率是多少？例如 $P(\text{Day 1 = Happy}) = 0.7, P(\text{Day 1 = Sad}) = 0.3$。
    *   **状态转移概率 (Transition Probability, A):** 精灵的心情会变化。如果今天开心，明天还开心的概率是多少？明天变伤心的概率是多少？如果今天伤心，明天变开心的概率是多少？明天还伤心的概率是多少？这描述了**隐藏状态之间的转换规律**。
        *   例子：
            *   $P(\text{Tomorrow=Happy} | \text{Today=Happy}) = 0.8$
            *   $P(\text{Tomorrow=Sad} | \text{Today=Happy}) = 0.2$
            *   $P(\text{Tomorrow=Happy} | \text{Today=Sad}) = 0.4$
            *   $P(\text{Tomorrow=Sad} | \text{Today=Sad}) = 0.6$
        *   **关键假设 (马尔可夫假设):** 精灵**明天的心情只取决于今天的心情**，跟昨天或更早的心情无关！这是 HMM 的核心简化假设。
    *   **发射概率 (Emission Probability, B):** 精灵的心情决定了她的活动。当她开心时，她唱歌、跳舞、哭泣的概率分别是多少？当她伤心时呢？这描述了**隐藏状态生成观测值的规律**。
        *   例子：
            *   $P(\text{Activity=Sing} | \text{Mood=Happy}) = 0.6$
            *   $P(\text{Activity=Dance} | \text{Mood=Happy}) = 0.3$
            *   $P(\text{Activity=Cry} | \text{Mood=Happy}) = 0.1$
            *   $P(\text{Activity=Sing} | \text{Mood=Sad}) = 0.1$
            *   $P(\text{Activity=Dance} | \text{Mood=Sad}) = 0.2$
            *   $P(\text{Activity=Cry} | \text{Mood=Sad}) = 0.7$
        *   **关键假设 (输出独立性假设):** 精灵**今天的活动只取决于她今天的心情**，跟昨天的心情或昨天的活动无关！这也是 HMM 的一个重要简化假设。

    **HMM 就是由这五元组定义的：$\lambda = (Q, V, \pi, A, B)$ （状态集，观测集，初始概率，转移概率，发射概率）。**

*   **数学直觉（重点理解概念）：**
    *   **状态序列 $I = (i_1, i_2, ..., i_T)$:** 我们看不见的隐藏状态序列 (e.g., Happy, Happy, Sad)。
    *   **观测序列 $O = (o_1, o_2, ..., o_T)$:** 我们能看到的观测值序列 (e.g., Sing, Dance, Cry)。
    *   **模型参数 $\lambda = (A, B, \pi)$:**
        *   $\pi = \{\pi_i\}$, $\pi_i = P(i_1 = q_i)$ (初始状态概率)
        *   $A = \{a_{ij}\}$, $a_{ij} = P(i_{t+1} = q_j | i_t = q_i)$ (状态转移概率矩阵)
        *   $B = \{b_j(k)\}$, $b_j(k) = P(o_t = v_k | i_t = q_j)$ (观测发射概率矩阵)
    *   **两个核心假设 (考试常考！):**
        1.  **齐次马尔可夫性假设 (Homogeneous Markov Assumption):** 隐藏状态的转移概率 $a_{ij}$ 只依赖于前一个状态 $q_i$ 和当前状态 $q_j$，与时间 $t$ 无关，也与更早的状态无关。
            $P(i_{t+1} | i_t, i_{t-1}, ..., i_1, o_t, ..., o_1) = P(i_{t+1} | i_t)$
        2.  **观测独立性假设 (Observation Independence Assumption):** 在时刻 $t$ 的观测值 $o_t$ 只依赖于该时刻的隐藏状态 $i_t$，与其他时刻的状态或观测值无关。
            $P(o_t | i_T, ..., i_1, o_{T}, ..., o_{t+1}, o_{t-1}, ..., o_1) = P(o_t | i_t)$
    *   **重要性：** 这两个假设**大大简化**了模型，使得计算成为可能。但也限制了模型的表达能力（比如无法考虑更长的上下文依赖，或者观测值之间可能存在的联系）。

### 怎么做（核心思想/算法步骤 + 关键公式与直观解释）

HMM 主要解决三个基本问题，对应三个经典算法：

1.  **问题一：评估 (Evaluation) - 计算观测序列概率**
    *   **问题：** 给定模型 $\lambda = (A, B, \pi)$ 和一个观测序列 $O = (o_1, ..., o_T)$，如何计算这个观测序列出现的**总概率** $P(O | \lambda)$？
    *   **用途：** 可以用来比较哪个 HMM 模型更能解释给定的观测数据。比如有两个不同的“天气精灵”模型，看哪个模型预测到“唱歌、跳舞、哭泣”这个序列的概率更大。
    *   **挑战：** 需要考虑所有可能的隐藏状态序列 $I = (i_1, ..., i_T)$，因为任何一条隐藏路径都可能生成观测序列 O。直接计算 $P(O|\lambda) = \sum_{I} P(O, I | \lambda) = \sum_{I} P(O|I,\lambda)P(I|\lambda)$ 是指数级的复杂度（如果状态数是 N，序列长 T，有 $N^T$ 条路径）。
    *   **解决方法：前向算法 (Forward Algorithm)**
        *   **思想：** 动态规划！定义一个**前向变量** $\alpha_t(i) = P(o_1, o_2, ..., o_t, i_t = q_i | \lambda)$，表示到时刻 $t$ 观测到序列 $o_1...o_t$ 并且**当前状态是 $q_i$** 的概率。
        *   **递推：** $\alpha_t(i)$ 可以由所有 $t-1$ 时刻的前向变量 $\alpha_{t-1}(j)$ 递推得到：
            $\alpha_t(i) = \left[ \sum_{j=1}^{N} \alpha_{t-1}(j) a_{ji} \right] b_i(o_t)$
            *   **直观解释：** 要在时刻 $t$ 到达状态 $q_i$ 并观测到 $o_t$，你必须在 $t-1$ 时刻处于某个状态 $q_j$ ($\alpha_{t-1}(j)$)，然后从 $q_j$ 转移到 $q_i$ ($a_{ji}$)，最后在状态 $q_i$ 发射出 $o_t$ ($b_i(o_t)$)。把所有可能的 $t-1$ 时刻状态 $q_j$ 的情况加起来。
        *   **最终结果：** $P(O | \lambda) = \sum_{i=1}^{N} \alpha_T(i)$ (在最后时刻 T，把所有可能状态的概率加起来)。
        *   **效率：** 时间复杂度约为 $O(N^2 T)$，远好于指数级。**考试重点：理解前向算法是用来高效计算给定观测序列出现概率的。**

2.  **问题二：解码 (Decoding) - 寻找最优隐藏状态序列**
    *   **问题：** 给定模型 $\lambda = (A, B, \pi)$ 和一个观测序列 $O = (o_1, ..., o_T)$，如何找到**最有可能**产生这个观测序列的那个**隐藏状态序列** $I^* = (i_1^*, ..., i_T^*)$？
        $I^* = \arg\max_{I} P(I | O, \lambda)$ (等价于找 $I$ 使得 $P(I, O | \lambda)$ 最大)
    *   **用途：** 这是 HMM 在 NLP 中最常用的功能！比如进行**词性标注**，就是要找到最可能的词性标签序列。
    *   **挑战：** 同样不能暴力搜索所有 $N^T$ 条路径。
    *   **解决方法：维特би算法 (Viterbi Algorithm)**
        *   **思想：** 还是动态规划！定义一个变量 $\delta_t(i)$ 表示到时刻 $t$，所有可能路径中，**以状态 $q_i$ 结尾**的那条路径的**最大概率**。同时记录一个指针 $\psi_t(i)$，记住这条最优路径在 $t-1$ 时刻是从哪个状态转移过来的。
            $\delta_t(i) = \max_{i_1,...,i_{t-1}} P(i_1,...,i_{t-1}, i_t=q_i, o_1,...,o_t | \lambda)$
        *   **递推：**
            $\delta_t(i) = \left[ \max_{1 \le j \le N} (\delta_{t-1}(j) a_{ji}) \right] b_i(o_t)$
            $\psi_t(i) = \arg\max_{1 \le j \le N} (\delta_{t-1}(j) a_{ji})$
            *   **直观解释：** 要找到达时刻 $t$ 状态 $q_i$ 的最优路径，需要看从 $t-1$ 时刻的哪个状态 $q_j$ 转移过来能使得 $\delta_{t-1}(j) \times a_{ji}$ 最大，然后乘上发射概率 $b_i(o_t)$。同时记下那个最优的 $j$。
        *   **最终结果：** 在时刻 T 找到最大的 $\delta_T(i)$，对应的 $i_T^*$ 就是最优路径的最后一个状态。然后根据指针 $\psi$ **回溯 (Backtracking)** 找到 $i_{T-1}^*, ..., i_1^*$，得到完整的 $I^*$。
        *   **效率：** 时间复杂度也是 $O(N^2 T)$。**考试重点：理解 Viterbi 算法是用来高效找到最可能的那条隐藏状态路径的，是 HMM 做预测（如 POS tagging）的核心。**

3.  **问题三：学习 (Learning) - 估计模型参数**
    *   **问题：** 给定一个观测序列 $O$ (或者很多观测序列)，如何**估计**出 HMM 的参数 $\lambda = (A, B, \pi)$，使得 $P(O | \lambda)$ 最大？
    *   **用途：** 从数据中**训练** HMM 模型。
    *   **挑战：** 隐藏状态是未知的，不能直接用简单的频率计数。
    *   **解决方法：鲍姆-韦尔什算法 (Baum-Welch Algorithm)**
        *   **思想：** 这是**期望最大化 (Expectation-Maximization, EM)** 算法的一个特例。它是一个迭代算法：
            1.  **E-step (期望步):** 基于当前的参数估计 $\lambda$，计算在给定观测 O 的条件下，模型在各个时刻 $t$ 处于状态 $q_i$ 的概率，以及在时刻 $t$ 从状态 $q_i$ 转移到 $q_j$ 的概率（利用前向变量 $\alpha$ 和后向变量 $\beta$ —— 类似于前向变量但从后往前算）。
            2.  **M-step (最大化步):** 利用 E-step 中计算出的期望（可以看作是状态和转移的“预期”或“软”计数），重新**估计**模型参数 $\pi, A, B$（类似于用这些预期计数来做最大似然估计）。
            3.  **重复** E-step 和 M-step 直到参数收敛。
        *   **数学直觉：** 因为不知道隐藏状态，所以先猜一下（E-step 算概率/期望），然后基于这个猜测去更新模型参数（M-step），再用更新后的参数去重新猜... 反复进行，模型参数会逐渐变好。**考试重点：理解 Baum-Welch 是用来从观测数据中学习 HMM 参数的，它是一种 EM 算法，通过迭代优化参数。**

*   **Mermaid 图示：HMM 结构示意**
    ```mermaid
    graph TD
        subgraph "Time t-1"
            H1(Hidden State i_{t-1}) -- Emission B --> O1(Observation o_{t-1});
        end
        subgraph "Time t"
            H2(Hidden State i_t) -- Emission B --> O2(Observation o_t);
        end
        subgraph "Time t+1"
            H3(Hidden State i_{t+1}) -- Emission B --> O3(Observation o_{t+1});
        end

        H1 -- Transition A --> H2;
        H2 -- Transition A --> H3;

        style H1 fill:#f9f,stroke:#333
        style H2 fill:#f9f,stroke:#333
        style H3 fill:#f9f,stroke:#333
        style O1 fill:#ccf,stroke:#333
        style O2 fill:#ccf,stroke:#333
        style O3 fill:#ccf,stroke:#333

        note for H1 "Depends only on i_{t-2} (Markov)"
        note for O2 "Depends only on i_t (Output Indep.)"
    ```

### 常见考试问题（如何应对）

1.  **Q: 什么是隐马尔可夫模型 (HMM)？请解释其核心组成部分（状态、观测、三个概率参数）。**
    *   **A (思路):**
        *   定义：一个关于时序的概率模型，包含隐藏状态和观测值。
        *   组成：
            *   **隐藏状态 (Q):** 系统内部的、不可见的真实状态。
            *   **观测值 (V):** 系统在每个状态下生成的、可见的输出。
            *   **初始状态概率 ($\pi$):** 系统开始时处于各个隐藏状态的概率。
            *   **状态转移概率 (A):** 从一个隐藏状态转移到另一个隐藏状态的概率（满足马尔可夫性）。
            *   **发射概率 (B):** 在某个隐藏状态下生成某个观测值的概率（满足观测独立性）。

2.  **Q: HMM 依赖哪两个核心假设？请解释它们的含义。这些假设有什么优点和局限性？**
    *   **A (思路):**
        *   **假设 1: 齐次马尔可夫性假设:** 当前隐藏状态只依赖于前一个隐藏状态。
        *   **假设 2: 观测独立性假设:** 当前观测值只依赖于当前隐藏状态。
        *   **优点:** 大大简化了模型，使得参数估计和问题求解（评估、解码、学习）在计算上可行（多项式时间复杂度）。
        *   **局限性:** 这两个假设在现实中往往不完全成立。
            *   马尔可夫性忽略了**长距离依赖**（比如，一个词的词性可能受句子前面很远地方的词影响）。
            *   观测独立性忽略了**观测之间的联系**或**状态对观测的复杂影响**（比如，一个词的出现可能受前面词的影响，而不仅仅是当前词性）。这限制了模型捕捉更复杂语言现象的能力。

3.  **Q: HMM 可以解决哪三个基本问题？简述每个问题的目标，以及通常使用哪种算法来解决。（不需要算法细节）**
    *   **A (思路):**
        1.  **评估 (Evaluation):**
            *   目标：计算给定观测序列出现的**概率** $P(O|\lambda)$。
            *   算法：**前向算法 (Forward Algorithm)**。
        2.  **解码 (Decoding):**
            *   目标：找到最有可能产生给定观测序列的**隐藏状态序列** $I^*$。
            *   算法：**维特比算法 (Viterbi Algorithm)**。
        3.  **学习 (Learning):**
            *   目标：根据观测数据**估计模型参数** $\lambda = (A, B, \pi)$。
            *   算法：**鲍姆-韦尔什算法 (Baum-Welch Algorithm)** (EM算法的一种)。

4.  **Q: 在词性标注 (POS Tagging) 任务中，HMM 的隐藏状态和观测值分别对应什么？该任务主要对应 HMM 的哪个基本问题？使用什么算法解决？**
    *   **A (思路):**
        *   **隐藏状态:** 词性标签 (Noun, Verb, Adj, Det, Prep, ...)。
        *   **观测值:** 句子中的单词 ("the", "cat", "sat", ...)。
        *   **对应问题:** **解码 (Decoding)** 问题，因为目标是找到最可能的词性标签序列。
        *   **使用算法:** **维特比算法 (Viterbi Algorithm)**。

5.  **Q: 假设一个 HMM 用于词性标注，状态有 DT (限定词) 和 NN (名词)。我们观察到序列 "the cat"。如果我们知道 $P(\text{DT}|\text{start})=0.8$, $P(\text{NN}|\text{start})=0.2$, $P(\text{NN}|\text{DT})=0.9$, $P(\text{DT}|\text{NN})=0.1$ (以及其他转移概率)，并且 $P(\text{"the"}|\text{DT})=0.6$, $P(\text{"cat"}|\text{NN})=0.5$ (以及其他发射概率)。请问，计算序列 "DT NN" 生成 "the cat" 的联合概率需要用到哪些参数？（写出表达式即可，不用计算）**
    *   **A (思路):**
        *   这个联合概率是 $P(I=\text{DT NN}, O=\text{"the cat"} | \lambda)$
        *   根据 HMM 的定义和假设，它可以分解为：
            $P(I=\text{DT NN}, O=\text{"the cat"} | \lambda) = P(i_1=\text{DT}|\text{start}) \times P(o_1=\text{"the"}|i_1=\text{DT}) \times P(i_2=\text{NN}|i_1=\text{DT}) \times P(o_2=\text{"cat"}|i_2=\text{NN})$
        *   需要用到的参数是：
            *   初始概率: $\pi_{\text{DT}} = P(\text{DT}|\text{start})$
            *   发射概率: $b_{\text{DT}}(\text{"the"}) = P(\text{"the"}|\text{DT})$
            *   转移概率: $a_{\text{DT, NN}} = P(\text{NN}|\text{DT})$
            *   发射概率: $b_{\text{NN}}(\text{"cat"}) = P(\text{"cat"}|\text{NN})$

### 真实任务案例（实际应用演示）

*   **词性标注 (POS Tagging) - 经典应用:**
    *   **场景:** 给定一个句子，如 "Book that flight."，需要为每个词标注词性。
    *   **HMM 应用:**
        1.  **建模:**
            *   **隐藏状态:** 词性标签集合 (e.g., {Noun, Verb, Det, ...})。
            *   **观测值:** 词汇表中的所有单词。
            *   **参数学习 (Learning):** 使用大量已经标注好词性的文本（如 Penn Treebank），运行 Baum-Welch 算法（或者如果数据已标注，可以直接用最大似然估计）来学习 HMM 参数：
                *   $\pi$: 句子通常以哪个词性开头的概率。
                *   $A$: 一个词性后面跟另一个词性的转移概率（如 $P(\text{Noun}|\text{Verb})$）。
                *   $B$: 某个词性下生成某个具体单词的发射概率（如 $P(\text{"book"}|\text{Noun})$, $P(\text{"book"}|\text{Verb})$）。
        2.  **预测 (Decoding):**
            *   拿到新句子 "Book that flight."。
            *   使用学习到的 HMM 参数 $\lambda$ 和 Viterbi 算法。
            *   Viterbi 算法会找到最可能的隐藏状态（词性）序列，比如可能是 "Verb Det Noun"。它会考虑到 "Book" 既可以是名词也可以是动词，但后面跟着限定词 "that" 和名词 "flight" 的模式更符合动词开头的句子结构（由转移概率 $A$ 体现），并且 "Book" 作为动词（发射概率 $B$）的可能性也存在。
    *   **效果:** HMM 在词性标注上能取得相当不错的效果（虽然现在不如 BiLSTM-CRF 或 BERT），因为它很好地捕捉了相邻词性间的依赖关系。

### 其它相关内容

*   **相关前置/后置概念:**
    *   **前置:** 概率论基础（条件概率、联合概率、贝叶斯定理）、马尔可夫链（HMM 的基础，只有状态转移）。
    *   **后置/相关:**
        *   **MEMM (Maximum Entropy Markov Models):** 对 HMM 的改进，是判别式模型，能使用更丰富的特征，但有标签偏见问题。
        *   **CRF (Conditional Random Fields):** 也是判别式模型，解决了 MEMM 的标签偏见问题，通常效果比 HMM/MEMM 好，但计算更复杂。CRF 在深度学习流行前是序列标注的事实标准。
        *   **RNN/LSTM/GRU/Transformer:** 深度学习模型，能自动学习特征，捕捉更长距离依赖，目前是序列建模的主流方法。但 HMM 的思想（状态、转移）仍有启发意义。
*   **HMM 的局限性:**
    *   **严格的独立性假设:** 马尔可夫性和观测独立性限制了模型能力。
    *   **特征表示能力弱:** HMM 本身不直接处理复杂的、重叠的特征。通常观测值是离散的符号，难以融入词嵌入等现代表示。
    *   **生成式模型 vs 判别式模型:** HMM 是生成式模型 ($P(O,I|\lambda)$)，它对联合概率建模。而像 CRF 这样的判别式模型直接对条件概率 $P(I|O,\lambda)$ 建模，在分类/标注任务上通常认为更有优势，因为它们可以直接关注区分不同标签的特征。
*   **发展趋势:** 纯粹的 HMM 在复杂 NLP 任务中已较少直接使用，但其思想被融入或启发了后续模型。例如，在语音识别中，HMM 仍可能与深度神经网络结合使用（DNN-HMM）。理解 HMM 有助于更好地理解序列建模的历史和挑战。

---

HMM 虽然是比较早期的模型，但它清晰地展示了如何用概率图模型来处理带有隐藏变量的序列问题。掌握它的基本原理、三个核心问题和对应算法，特别是 Viterbi 算法在解码（预测）中的作用，对于理解序列标注等任务的早期解决方案非常有价值！