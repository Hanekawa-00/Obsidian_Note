# 基础
## MySQL性能差的原因
- 为使用索引进行优化
- 查询数据量过多（如全表扫描）
- CPU等硬件资源瓶颈
- SQL语句过于复杂
### Q 
- 使用索引优化
- 添加Redis等缓存
---
## 多表合并查询
1. **Q: 内连接(inner join)和左外连接(left join)有什么区别？举例说明。**
    - **A:** 内连接只返回两个表都匹配的行（交集），左外连接返回左表所有行以及右表匹配的行（左表为主）。举例：查询“哪些客户下过订单”用内连接；查询“所有客户及其订单（即使没下过订单）”用左外连接。
2. **Q: 什么时候用右外连接？它和左外连接能互相转换吗？**
    - **A:** 右外连接以右表为主，返回右表所有行及左表匹配行。例如，查询“所有订单及其对应客户（即使客户不存在）”。左右外连接可以互相转换，只需交换 FROM 和 JOIN 后面的表位置即可。
3. **Q: 交叉连接有什么用？在实际中常用吗？**
    - **A:** 交叉连接返回笛卡尔积，通常不直接用于常规数据查询，因为它结果集巨大。常见用途：生成所有可能的组合（如生成所有商品和所有仓库的组合）、测试数据生成。
4. **Q: 如何优化 JOIN 操作的性能？**
    - **A:**
        - **在连接列上创建索引：** 这是最重要的优化手段。
        - **选择合适的连接类型：** 避免不必要的全表扫描。
        - **优化连接顺序：** 数据库会尝试优化，但有时手动调整 FROM 后面的表顺序（小表在前）可能有效。
        - **减少连接的表数量：** 如果可能，避免连接过多表。
        - **使用小表驱动大表：** 在某些场景下，让结果集较小的表先进行筛选，再与大表连接。
5. **Q: 为什么 MySQL 不直接支持 FULL OUTER JOIN？如何模拟？**
    - **A:** MySQL 早期版本设计如此。模拟方法是使用 LEFT JOIN 和 RIGHT JOIN 的结果集通过 UNION 操作合并。UNION 会自动去重。

![[sql.svg]]
## 三大范式
1. **第一范式 (1NF)：**
    - **原理：** 确保数据库表中的每一列都是 **原子性的**，不可再分。
    - **比喻：** 就像你的 Excel 表格里，一个单元格不能包含多个值。比如，“联系电话”列不能同时写“家里电话: 123, 手机: 456”，而是应该拆分成“家里电话”和“手机”两列，或者更进一步，创建一个单独的“联系方式”表。
    - **要求：** ==每列的值都是不可分割的最小单元。==
    - **示例（不符合 1NF）：** 一个“学生课程成绩”表，其中有一列是“课程列表”，值为“数学, 物理, 英语”。
    - **示例（符合 1NF）：** 将“课程列表”拆分成多行，每行只包含一门课程的信息。
2. **第二范式 (2NF)：**
    - **原理：** 在满足 1NF 的基础上，非主键列必须 **完全依赖于主键**，而不是主键的一部分。
    - **比喻：** 就像你的 Excel 表格，如果你的主键是“学号 + 课程ID”，那么学生的姓名、班级等信息（非主键列）应该只依赖于“学号”（主键的一部分），而不是同时依赖于“学号 + 课程ID”。如果学生姓名依赖于“学号 + 课程ID”，那就意味着同一个学生选不同课时，他的姓名可能会重复存储，且修改名字时需要在多处修改。
    - **要求：** 满足 1NF；非主键列必须完全函数依赖于主键。
    - **示例（不符合 2NF）：** “学生选课成绩”表 (学号, 课程ID, 学生姓名, 课程名称, 成绩)。主键是 (学号, 课程ID)。学生姓名只依赖于学号，课程名称只依赖于课程ID，它们没有完全依赖于联合主键 (学号, 课程ID)。
    - **示例（符合 2NF）：** 将表拆分成三个表：
        - `学生表` (学号, 学生姓名, 班级) - 主键：学号
        - `课程表` (课程ID, 课程名称) - 主键：课程ID
        - `选课成绩表` (学号, 课程ID, 成绩) - 主键：(学号, 课程ID)，外键：学号引用学生表，课程ID引用课程表。 这样，学生姓名只存储在学生表中，课程名称只存储在课程表中，避免了冗余和更新异常。
3. **第三范式 (3NF)：**
    - **原理：** 在满足 2NF 的基础上，非主键列之间 **不能存在传递依赖**(同一个表中存在其他主键依赖)。也就是说，==非主键列不能依赖于另一个非主键列。==
    - **比喻：** 就像你的 Excel 表格，如果你的主键是“学号”，非主键列有“班级编号”和“班主任姓名”。如果班主任姓名只依赖于班级编号（而不是直接依赖于学号），那么就存在传递依赖：学号 -> 班级编号 -> 班主任姓名。这意味着同一个班级的所有学生都会重复存储班主任姓名，且班主任换人时需要在多处修改。
    - **要求：** 满足 2NF；任何非主键列都不对非主键列具有传递依赖。
    - **示例（不符合 3NF）：** 在符合 2NF 的 `学生表` 中 (学号, 学生姓名, 班级编号, 班主任姓名)。主键是学号。班主任姓名依赖于班级编号，班级编号依赖于学号 (学号 -> 班级编号 -> 班主任姓名)。
    - **示例（符合 3NF）：** 将表拆分成两个表：
        - `学生表` (学号, 学生姓名, 班级编号) - 主键：学号，外键：班级编号引用班级表。
        - `班级表` (班级编号, 班主任姓名) - 主键：班级编号。 这样，==班主任姓名只存储在班级表中，避免了冗余和更新异常。==
---
1. **Q: 什么是数据库三大范式？各自解决什么问题？**
    
    - **A:** 范式是数据库设计原则，用于减少冗余和异常。1NF 要求列原子化；2NF 要求非主键完全依赖主键；3NF 要求非主键无传递依赖。分别解决列不可分、部分依赖、传递依赖带来的冗余和异常。
2. **Q: 为什么有时要“反范式”设计？会带来什么问题？**
    
    - **A:** 反范式是为了提高查询性能，通过增加冗余来减少 JOIN 操作。例如，在订单表中存储冗余的客户姓名，查询时无需连接客户表。问题：增加数据冗余、更新异常风险、维护成本高。提示：在面试中，强调“范式与性能的权衡”。
3. **Q: 如何判断一个表是否符合某个范式？**
    
    - **A:** 检查是否满足该范式及之前所有范式的要求。例如，判断是否符合 3NF，先看是否符合 1NF、2NF，再看是否存在传递依赖。
4. **Q: 数据库设计到哪个范式比较合适？**
    
    - **A:** 通常到 3NF 是一个很好的平衡点，既减少了冗余，又不会导致表过多、连接过于复杂。BCNF 更严格，但在一些场景可能引入新的复杂性。
5. **Q: 在实际项目中，你是如何进行数据库设计的？**
    
    - **A:** （结合范式和实际经验回答）先进行需求分析，识别实体和关系 -> 遵循范式原则设计表结构（通常到 3NF）-> 考虑业务场景和性能需求，可能进行适当的反范式优化 -> 定义主键、外键、索引 -> 进行数据库建模和评审。
## IN和EXISTS子查询
>这种查询一般只是判断数据是否存在的查询方法，要得到复杂的数据还是使用`join`合并查询
1. **`IN` 的原理：**
    - **执行过程：** 通常情况下，数据库会==先执行子查询，生成一个结果集列表==（例如，所有下过订单的客户 ID 列表）。然后，数据库遍历外层查询的每一行，检查该行中用于匹配的列值（例如，客户 ID）是否存在于子查询生成的结果集列表中，存在则添加到返回结果中。
    - **实例：** 就像你拿到一份“所有下过订单的客户 ID”的清单（子查询结果集），然后挨个核对“客户名单”上的每一个客户，看看他们的 ID 是否在你的清单上。如果在，就把这个客户的信息挑出来。
    - **特点：** 子查询会执行一次，生成一个完整的列表。适用于子查询结果集较小的情况。
2. **`EXISTS` 的原理：**
    - **执行过程：** `EXISTS` 子查询通常是 **关联子查询**。数据库会==遍历外层查询的每一行。对于外层查询的每一行，都会执行一次子查询。子查询会判断是否存在与当前外层查询行相关联的记录。==只要子查询找到匹配的 **第一行**，就会返回 TRUE，外层查询继续处理下一行；如果子查询遍历完所有相关记录都没有找到匹配的行，就返回 FALSE。
    - **实例：** 就像你拿着“客户名单”上的第一个客户，去“订单列表”里找“这个客户有没有订单”。找到了一个订单？好的，不用再找了，这个客户符合条件，把他挑出来，然后处理下一个客户。如果找遍了都没找到这个客户的订单，就放弃这个客户，处理下一个。
    - **特点：** 子查询会根据外层查询的行数执行多次。适用于子查询结果集较大，或者子查询本身包含复杂的逻辑判断（如 JOIN、WHERE）的情况。
### Q
1. **Q: IN 和 EXISTS 的主要区别是什么？**
    
    - **A:** `IN` 是将外层查询的值与子查询的结果集进行匹配，子查询通常先执行一次；`EXISTS` 是判断子查询是否返回行，子查询通常是关联子查询，对外层查询的每一行执行一次。
2. **Q: 什么时候优先使用 IN，什么时候优先使用 EXISTS？**
    
    - **A:**
        - **IN 优先：** **子查询结果集较小**，或者子查询不依赖外层查询的任何列。
        - **EXISTS 优先：** 子查询结果集较大，或者子查询依赖外层查询的列（关联子查询），且外层查询结果集相对较小。
        - **最准确判断：** 使用 `EXPLAIN` 分析查询计划，看哪种写法的执行计划更优。
3. **Q: EXISTS 后面的 SELECT 1 是什么意思？可以写 SELECT * 吗？**
    
    - **A:** `SELECT 1` 是一种约定俗成的写法，表示“只要找到任何一行就行，具体值是多少不重要”。写 `SELECT *` 也可以，但 `SELECT 1` 效率更高，因为它不需要实际去检索列的值。
4. **Q: IN 和 JOIN 能互相替代吗？有什么优缺点？**
    - **A:** 在很多情况下可以互相替代。
        - **JOIN 优点：** 可以返回两个表中的所有列，更灵活；优化器通常对 JOIN 有更好的优化。
        - **JOIN 缺点：** 在某些场景下，如果只关心是否存在，JOIN 可能会产生冗余的中间结果集，不如 EXISTS 直接。
        - **IN 优点：** 语法简洁，易于理解。
        - **IN 缺点：** 在大数据集上性能可能不如 EXISTS 或优化后的 JOIN。
        - **EXISTS 优点：** 在大数据集上可能性能更好，尤其适合判断是否存在。
        - **EXISTS 缺点：** 只能判断是否存在，不能直接获取子查询中的其他列信息。
5. **Q: IN 子查询中如果返回 NULL 值会有什么影响？**
    - **A:** 如果子查询返回的列表包含 NULL 值，那么 `WHERE column IN (..., NULL, ...)` 这样的条件可能会导致意外的结果。因为 `value IN (..., NULL, ...)` 对于任何非 NULL 的 `value` 都会判断为 UNKNOWN（未知），导致无法匹配。而 `WHERE column NOT IN (..., NULL, ...)` 对于任何值都会判断为 UNKNOWN，导致没有任何行返回。这是一个常见的陷阱。**而 `EXISTS` 子查询通常不受 NULL 值影响。**
## SQL执行顺序
```sql
SELECT DISTINCT column_list
FROM table_name 
[JOIN ...]
WHERE condition
GROUP BY grouping_column(s)
HAVING group_condition
ORDER BY sorting_column(s)
LIMIT limit_value [OFFSET offset_value];
```

1. **`FROM`**: 确定数据来源。首先，确定需要从哪些表获取数据。如果涉及多个表，会进行笛卡尔积操作（虽然优化器通常会避免直接计算完整的笛卡尔积）。
    
2. **`JOIN`**: 表连接。根据 JOIN 条件将 FROM 子句中指定的表连接起来，生成一个更大的中间结果集。
    
3. **`ON`**: JOIN 的连接条件。在 JOIN 过程中，根据 ON 子句指定的条件进行匹配。
    
4. **`WHERE`**: 行过滤。根据 WHERE 子句指定的条件，从 JOIN 生成的中间结果集中筛选出符合条件的行。**注意：** **在这个阶段，SELECT 子句中的列别名是不可用的，因为 SELECT 子句还没执行。(只是构建了语句，或者说是查询逻辑)**
    
5. **`GROUP BY`**: 分组。根据 GROUP BY 子句指定的列，将 WHERE 过滤后的结果集进行分组。
    
6. **`CUBE / ROLLUP` (可选):** 更高级的分组操作，生成更详细的聚合结果。
    
7. **`HAVING`**: 分组后过滤。对 GROUP BY 分组后的结果进行过滤。与 WHERE 不同，HAVING 可以使用聚合函数（如 COUNT(), SUM(), AVG()）作为过滤条件。
    
8. **`SELECT`**: 选择列。根据 SELECT 子句指定的列，从 HAVING 过滤后的结果集中选择需要显示的列。此时，可以在 SELECT 列表中使用列别名。
    
9. **`DISTINCT` (可选):** 去重。如果指定了 DISTINCT，则从 SELECT 选出的结果集中移除重复的行。
    
10. **`ORDER BY`**: 排序。根据 ORDER BY 子句指定的列对最终结果集进行排序。此时，可以使用 SELECT 子句中定义的列别名。
    
11. **`LIMIT / OFFSET` (或 TOP, ROWNUM 等):** 分页。根据 LIMIT 和 OFFSET 子句（或其他数据库系统的等效子句），限制返回的行数，用于实现分页。这个通常是最后执行的步骤。
### 实例

1. 假设我们有 `orders` 表: `order_id`, `user_id`, `order_amount`, `order_date`。
	**需求：** 统计每个用户(`group by user_id`)的订单总金额，只统计 2023 年的订单，并且只显示订单总金额大于 1000 元的用户，最后按总金额降序排列，取前 10 条记录。
```sql
	SELECT user_id, SUM(order_amount) AS total_amount -- 8. SELECT (选择列，可以使用聚合函数和别名)
	FROM orders -- 1. FROM (指定数据来源)
	WHERE order_date >= '2023-01-01' AND order_date < '2024-01-01' -- 4. WHERE (行过滤，发生在分组前)
	GROUP BY user_id -- 5. GROUP BY (按用户分组)
	HAVING SUM(order_amount) > 1000 -- 7. HAVING (分组后过滤，可以使用聚合函数)
	ORDER BY total_amount DESC -- 10. ORDER BY (按总金额降序排序，可以使用 SELECT 中的别名)
	LIMIT 10; -- 11. LIMIT (限制返回行数)
	```
2. **场景：** 社交平台需要找出粉丝数量超过 10000 的用户，并按粉丝数量降序显示前 50 名。
*   **问题：** 需要对用户进行分组（每个用户一组），计算粉丝总数，然后过滤，最后排序和分页。
*   **执行顺序应用：**

    假设有 `followers` 表 (follower_id, followed_user_id)。

    **需求：** 查询粉丝数大于 10000 的用户，并按粉丝数降序排列前 50 名。

    ```sql
    SELECT followed_user_id, COUNT(follower_id) AS follower_count -- 7. SELECT (选择用户ID和粉丝数)
    FROM followers -- 1. FROM (指定数据来源表)
    -- 这里没有 WHERE 子句，因为没有行级别的过滤需求
    GROUP BY followed_user_id -- 5. GROUP BY (按被关注用户分组)
    HAVING COUNT(follower_id) > 10000 -- 6. HAVING (过滤粉丝数大于10000的组)
    ORDER BY follower_count DESC -- 8. ORDER BY (按粉丝数降序排序，使用 SELECT 中的别名)
    LIMIT 50; -- 9. LIMIT (限制返回前50条)
    ```

    **执行流程：**
    1.  从 `followers` 表中获取所有数据。
    2.  将数据按 `followed_user_id` 分组。
    3.  对每个组（每个被关注用户），计算其粉丝数量 (`COUNT(follower_id)`)。
    4.  过滤掉粉丝数量小于等于 10000 的组。
    5.  选择每个符合条件的组的 `followed_user_id` 和计算出的粉丝数量。
    6.  按粉丝数量降序排列结果。
    7.  取出前 50 条记录。

    **效果：** 通过遵循 SQL 的执行顺序，我们能够清晰地表达业务逻辑，并让数据库按照高效的方式执行查询（先分组聚合，再过滤）。
# 数据库架构
## 更新语句执行流程
一条典型的 UPDATE 语句的执行过程涉及多个组件和步骤：
1. **连接器 (Connector):** 客户端与数据库建立连接。
2. **查询缓存 (Query Cache - 8.0 版本后已移除):** 在旧版本中，如果查询语句在缓存中命中，直接返回结果。但对于更新语句，由于会修改数据，缓存通常会失效。
3. **分析器 (Parser):** 对 SQL 语句进行词法分析和语法分析，检查语句是否符合 SQL 语法规范。
4. **优化器 (Optimizer):** 分析 SQL 语句，生成一个最优的执行计划。对于 UPDATE 语句，优化器会决定如何找到需要更新的行（例如，是否使用索引）、以什么顺序进行更新等。
5. **执行器 (Executor):** 根据优化器生成的执行计划，调用存储引擎接口执行具体的更新操作。
6. ==**存储引擎 (Storage Engine - InnoDB):** 负责数据的存储和管理。更新操作最终由存储引擎完成。在 InnoDB 中，更新过程通常涉及以下关键步骤：==
    - **获取锁 (Locking):**
        - 执行器首先会根据 WHERE 条件找到需要更新的行。
        - 为了保证数据的一致性（尤其是在并发环境下），InnoDB 会对这些行加 **排他锁 (X Lock)**。排他锁会阻塞其他事务对这些行的读写操作。
        - 如果==更新语句没有 WHERE 条件，或者 WHERE 条件无法利用索引扫描，可能会进行全表扫描==，并对扫描到的行逐行加锁，甚至在某些情况下加表锁。
        - 如果需要更新的行已经被其他事务加锁，当前事务会进入等待状态，直到获取到锁。
    - **读取数据到内存 (Read Data):** 将需要更新的行从磁盘读取到 InnoDB 的缓冲池 (Buffer Pool) 中。
    - **修改内存中的数据 (Modify Data):** 在缓冲池中修改这些行的数据。
    - **写入 Undo Log (Write Undo Log):** 在修改数据之前，InnoDB 会将==原始数据（旧版本）==写入到 Undo Log 中。Undo Log 用于==事务回滚==。如果事务失败，可以使用 Undo Log 将数据恢复到修改前的状态。
    - **写入 Redo Log Buffer (Write Redo Log Buffer):** 将数据修改的操作（物理操作，记录“在哪个页的哪个偏移量修改了什么值”）写入到 Redo Log Buffer 中。Redo Log 用于==事务持久化和崩溃恢复==。即使数据还没刷到磁盘，只要 Redo Log 记录了，系统崩溃后也能通过 Redo Log 恢复数据。
    - **Redo Log Buffer 刷盘 (Redo Log Buffer Flush):** 根据事务的提交策略（`innodb_flush_log_at_trx_commit` 参数），Redo Log Buffer 中的内容会在事务提交前或提交时被刷写到磁盘的 Redo Log 文件中。这是保证事务持久性的关键步骤。
    - **写入 Binlog (Write Binlog - 如果开启):** 如果数据库开启了二进制日志 (Binlog)，执行器会将在存储引擎层==执行成功的修改操作==（逻辑操作，记录“执行了哪条 SQL 语句”）写入到 Binlog Buffer 中。Binlog 主要用于主从复制和数据恢复。
    - **Binlog Buffer 刷盘 (Binlog Buffer Flush):** Binlog Buffer 中的内容会在事务提交时被刷写到磁盘的 Binlog 文件中。Binlog 的刷盘策略由 `sync_binlog` 参数控制。
    - **提交事务 (Commit Transaction):** 如果一切顺利，事务会被提交。提交操作会标记事务的结束，并释放事务期间持有的锁。在两阶段提交 (Two-Phase Commit - 2PC) 中，Binlog 和 Redo Log 的刷盘顺序和时机会有额外的协调步骤，以保证主从一致性。
    - **脏页刷盘 (Dirty Page Flush):** 在事务提交后，缓冲池中被修改的页面（脏页）会在后台择机刷写到磁盘的数据文件中。这个过程是异步的，不一定在事务提交时立即发生，但 Redo Log 保证了即使在刷盘前系统崩溃，数据也能恢复。
        
## 段区分页
从大到小，InnoDB 存储数据的层级结构可以概览为：表空间 (Tablespace) -> 段 (Segment) -> 区 (Extent) -> 页 (Page) -> 行 (Row)。

1. **行 (Row)**
    
    - **概念**：行是数据库表中的一条记录。它是数据最基本的单位，代表一个独立的实体或事实。InnoDB 采用**行存储方式**，意味着数据是按照行进行组织和管理的。
    - **细节**：行数据可能有不同的格式，例如 COMPACT, REDUNDANT, DYNAMIC 等。MySQL 8.0 默认的行格式是 DYNAMIC。在 InnoDB 中，每条记录（行）会包含一些隐藏列，例如 DB_TRX_ID（记录修改该行的事务 ID）和 DB_ROLL_PTR（指向 Undo Log 中的前一个版本）。如果表没有主键，还会有一个 DB_ROW_ID 用于唯一标识该行。
    - **与索引的关系**：对于 InnoDB 的聚簇索引（也叫主索引），B+ 树的叶子节点直接存储**完整的用户记录**，即行数据。其他辅助索引的叶子节点则存储相应记录的主键值。
2. **页 (Page)**
    
    - **概念**：页是 **InnoDB 存储数据的基本单元**，也是 **磁盘 I/O 的基本单位**。数据库每次读写操作都**至少以 16 KB 为单位**进行。
    - **作用**：引入页的主要目的是**减少磁盘 I/O 次数，提高数据访问效率**。一次读取一个页可以获取多行数据。
    - **细节**：页的标准大小通常是 **16 KB**。在索引结构中，**索引树上的一个节点就是一个页**。B+ 树索引利用了页的物理连续性来实现高效的范围查询和顺序扫描。
    - **与上层的关系**：页组成了更大的单位，称为区。
3. **区 (Extent)**
    
    - **概念**：区是 **一组连续的页**。通常包含 **64 个连续的页**。由于每个页是 16KB，所以一个区的大小是 64 * 16KB = **1MB**。
    - **作用**：使用区而不是单独的页进行数据分配，可以优化磁盘操作，**减少磁盘寻道时间**，特别是在大量数据进行读写时，可以**提高连续空间的分配效率并减少碎片**。它是空间分配的基本单位。
    - **与上层的关系**：区组成了更大的单位，称为段。
4. **段 (Segment)**
    
    - **概念**：段是由一个或多个区组成的。
    - **细节**：常见的段有**数据段、索引段、回滚段**等。
        - **数据段**：用于存储叶子节点中的数据。
        - **索引段**：用于存储非叶子节点的数据（索引页）。
        - **回滚段 (Rollback Segment)**：包含了事务执行过程中用于数据回滚的旧数据（Undo Log）。
    - **与下层的关系**：每个段由一个或多个区组成。
    - **与上层的关系**：表空间由多个段组成。

**层级关系总结**： 表空间 (Tablespace) -> 段 (Segment) -> 区 (Extent) -> 页 (Page) -> 行 (Row) 即：**表空间**包含**段**，**段**包含**区**，**区**包含**页**（通常是 64 个连续的页），**页**包含**行**。

**重要性与关联**：

- **索引原理**：B+ 树索引的节点就是**页**。非叶子节点页存储索引键值和指向下一层页的指针；叶子节点页（对于聚簇索引）存储**完整的行记录**。理解页的概念是理解 B+ 树索引工作方式的基础.
- **磁盘 I/O**：数据库读写操作以**页**为单位。优化器的成本计算会考虑 IO 成本，即从磁盘读取数据到内存的开销，这与需要读取的页数量直接相关。
- **空间管理**：通过以**区**为单位分配和管理空间，InnoDB 提高了效率并减少了碎片。
- **Buffer Pool**：InnoDB 的缓冲池（Buffer Pool）缓存的就是**数据页和索引页**。理解页的概念有助于理解 Buffer Pool 的工作原理。
- **日志和事务**：Redo Log 主要记录物理操作（例如在某个**页**的某个偏移量修改），Undo Log 记录数据修改前的**行**信息。事务和恢复机制与页级别的数据修改密切相关。锁通常作用于**行**或**页**，但底层实现与页结构有关.

通过这些层级结构，InnoDB 能够高效地组织、存储和管理大量数据，并支持复杂的索引、事务和并发控制机制。
# 存储引擎
## 如何选择存储引擎
**大多数情况下，使用默认的 InnoDB 存储引擎是最好的选择**。这是因为 InnoDB 提供了许多重要的特性，使其适用于绝大多数的业务场景：
- **成熟稳定，功能完善**。
- **事务支持优秀**。InnoDB 支持事务，能够保证数据库操作的 **原子性、一致性、隔离性和持久性 (ACID)**。
- **支持行级锁**，而不是表级锁，这有助于**提高并发性能**。MVCC (Multiversion concurrency control) 也可以看作是行级锁的一个升级，能有效减少加锁操作，提高性能，而 InnoDB 支持 MVCC。
- **支持外键**。外键有助于**维护数据一致性**，尽管在实际生产中通常==不建议在数据库层面使用外键，而是在应用层面进行约束==。
- **支持数据库异常崩溃后的安全恢复 (crash-safe)**。InnoDB 的崩溃恢复能力依赖于 redo log。即使数据库发生异常重启，之前提交的记录都不会丢失。
- **默认使用 REPEATABLE-READ 隔离级别并不会有任何性能损失**。实际上，InnoDB 实现的 REPEATABLE-READ 隔离级别可以解决幻读问题。

除了 InnoDB，MySQL 还支持其他存储引擎，其中比较常见的有 MyISAM 和 Memory：

- **MyISAM**：MyISAM 不支持事务、崩溃恢复和 MVCC。尽管在某些读密集的情况下使用 MyISAM 也是合适的，但其不支持事务和崩溃恢复等缺点通常是我们在日常业务系统中需要介意的。不要轻易相信“MyISAM 比 InnoDB 快”之类的经验之谈，在很多场景中，特别使用了聚簇索引或需要访问的数据都可以放入内存的应用中，InnoDB 的速度可以超越 MyISAM。
- **MEMORY**：Memory 存储引擎将数据存放在内存中，因此速度非常快。它适合用于临时表或数据量不大的情况。

**总结来说，对于日常开发的业务系统，几乎找不到什么理由不使用默认的 InnoDB 存储引擎**。InnoDB 提供的事务、崩溃恢复、行级锁等能力对于保证数据完整性和高并发应用至关重要。MyISAM 可能适用于特定的**读多写少且对事务和崩溃恢复没有要求**的场景，但这种情况相对较少。Memory 则适用于对读写速度要求极高且数据不需要持久化的临时场景。
# 日志
## Binlog
**Binlog 是 MySQL 的二进制日志，属于 MySQL 的 Server 层，独立于存储引擎**。它记录了所有对数据库进行更改的操作，但不包括像 `SELECT` 和 `SHOW` 这类的查询操作。Binlog 的主要作用包括**数据恢复、主从复制、主主复制以及主从同步**。

**Binlog 的作用和用途：**

- **数据恢复：** 如果误删了数据，可以使用 binlog 回退到误删之前的状态. MySQL 提供了 `mysqlbinlog` 工具来查看 binlog 文件的内容，可以用于恢复数据、查看数据变更等.
- **主从复制：** Binlog 是实现 MySQL 主从同步的基础. 从库（slave）会读取主库（master）的 binlog 来进行数据同步.
    - 主库将数据改变记录到 binlog 中.
    - 从库连接主库请求 binlog，主库创建一个 log dump 线程发送 binlog 内容.
    - 从库的 IO 线程接收 binlog 内容，保存到中继日志（relay log）中.
    - 从库的 SQL 线程读取 relay log 并解析执行，实现数据一致.
- **数据备份和同步：** 数据库的数据备份、主备、主主、主从都离不开 binlog，需要依靠 binlog 来同步数据，保证数据一致性.

**Binlog 的记录格式：**

MySQL 提供了三种 binlog 格式:

- **STATEMENT 格式：** 记录的是主库数据库的写指令（SQL 语句）. 这种格式性能较高，但对于像 `now()` 这样的函数或者获取系统参数的操作，可能导致主从数据不同步的问题.
- **ROW 格式（默认）：** 记录的是主库数据库修改后的具体数据. 这种格式能够保证同步数据的一致性，解决了使用 STATEMENT 格式时可能出现的数据不一致问题. 通常情况下都指定为 ROW 格式，因为它为数据库的恢复与同步带来了更好的可靠性. 但这种格式需要更大的容量来记录，比较占用空间，恢复与同步时会更消耗 IO 资源，影响执行速度. 大批量写操作时会产生大量的日志，因为 row 格式会记录每一行数据的修改.
- **MIXED 格式：** 是 STATEMENT 和 ROW 两种格式的混合使用. MySQL 会判断这条 SQL 语句是否可能引起数据不一致，如果是，就用 ROW 格式，否则就用 STATEMENT 格式.

可以通过 `binlog_format` 参数配置 binlog 格式.

**Binlog 的写入机制：**

Binlog 的写入时机非常简单. 事务执行过程中，先把日志写到 binlog cache. 事务提交的时候，再把 binlog cache 写到 binlog 文件中. 为了确保一个事务的 binlog 不会被拆开，无论事务多大，也要确保一次性写入. 系统会给每个线程分配一个块内存作为 binlog cache.

写入 binlog 时，数据会先写入缓存，当缓冲区满了再由操作系统将数据一次性刷入磁盘. 

## 两阶段提交机制
**阶段 1: 准备阶段 (Prepare Phase)**
1. **写入 Redo Log (Prepare 状态):** InnoDB 存储引擎会将事务的所有修改操作写入 Redo Log Buffer，并将事务状态标记为“准备好提交”(**Prepared**)。然后，Redo Log Buffer 中的数据会被刷写到磁盘的 Redo Log 文件中（但此时事务在 InnoDB 层面还未正式提交）。这个刷盘操作是强制性的，以确保即使系统崩溃，Redo Log 也记录了事务的修改。
**阶段 2: 提交阶段 (Commit Phase)**

2. **写入 Binlog:** 事务协调器（通常是 Binlog 模块）会生成这个事务对应的 Binlog 事件，并写入 Binlog Buffer。然后，Binlog Buffer 中的数据会被刷写到磁盘的 Binlog 文件中。Binlog 的刷盘策略由 `sync_binlog` 参数控制。
    
3. **提交 InnoDB 事务:** Binlog 刷盘成功后，协调器会通知 InnoDB 存储引擎正式提交事务。InnoDB 会在内存中完成事务的提交操作（比如释放锁），并更新事务的状态为“已提交”。
    
4. **清理工作:** 事务完成后，会进行一些清理工作，比如释放锁、清理 Undo Log 等。
    
**异常处理：**
- **如果在准备阶段（写入 Redo Log 后，写入 Binlog 前）系统崩溃：** Redo Log 中记录了事务的修改，但 Binlog 中没有。恢复时，数据库会发现 Redo Log 中的事务处于“准备好提交”状态，==但 Binlog 中没有对应的记录==，此时会认为这个事务未提交，进行回滚（利用 Undo Log）。数据恢复到崩溃前的状态，不会丢失数据。
- **如果在提交阶段（写入 Binlog 后，提交 InnoDB 事务前）系统崩溃：** Redo Log 中记录了事务的修改并处于“准备好提交”状态，Binlog 中也记录了事务。恢复时，数据库会发现 Redo Log 中的事务处于“准备好提交”状态，==并且在 Binlog 中找到了对应的记录==，此时会认为这个事务已经提交，会继续完成 InnoDB 的提交操作（即使数据页还没刷盘，也可以通过 Redo Log 恢复）。数据会被正确提交。

**为什么需要 Redo Log 的 Prepare 状态？** 在阶段 1 写入 Redo Log 时，事务被标记为 Prepared。这个状态非常关键。如果在 Binlog 刷盘前崩溃，恢复时通过 Redo Log 看到 Prepared 状态但 Binlog 中没有记录，就知道这个事务没有完成，需要回滚。如果在 Binlog 刷盘后崩溃，恢复时通过 Binlog 找到对应的记录，再通过 Redo Log 看到 Prepared 状态，就知道这个事务应该被提交。这个 Prepared 状态是连接 Redo Log 和 Binlog 的“桥梁”。